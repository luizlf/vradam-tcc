## Adam vs SGD - On Kaggle's Titanic Dataset
A 3-layer neural network with SGD and Adam optimizers built with numpy.


### Introduction
This is a response to Siraj Raval's [Coding Challenge](https://github.com/llSourcell/The_evolution_of_gradient_descent/) to implement the Adam Optimization Strategy. In this notebook, we are building a 3-layer neural network with numpy for the [Kaggle Titanic Dataset](https://www.kaggle.com/c/titanic/data), and comparing the performance difference between a standard Stochastic Gradient Descent and Adam.


### Requirements
* numpy
* pandas
* matplotlib

### Usage
Run `jupyter notebook` in your Python 3 conda environment

### With reference to:
1. [Adam: A method for Stochastic Optimization](https://arxiv.org/abs/1412.6980) by Diederik P. Kingma, Jimmy Ba  
2. [CS231: Neural Networks](http://cs231n.github.io/neural-networks-3/#update) by Andrej Karpathy
3. [Optimizing Gradient Descent](http://sebastianruder.com/optimizing-gradient-descent/index.html#adam) by Sebastian Ruder
